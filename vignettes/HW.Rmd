---
title: "Homework For StatComp"
author: "22001"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Introduction to StatComp}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

###A-22001-2022-09-09


## Question

Use knitr to produce at least 3 examples (texts, figures, tables)

## Answer

1.There is a time series data:

```{r echo=FALSE}
rm(list=ls())
library(TSA)
#  Nonstationarity
```

```{r}
data(oil.price)
plot(log(oil.price), ylab='Price per Barrel',type='l')
```
The plots show that this time series is nonstationary,so we may consider difference.

We choose the first differenc for the data, we can see that:

```{r echo=FALSE}
plot(diff(log(oil.price)), ylab='Price per Barrel',type='l')
```
With first difference , it shows that this time series become more stationary . And consider the plot of 'eacf' , we can choose MA(1) model.

2.There is a table about the data of artists:
```{r}
knitr::kable(head(trees))
```
3.R is a free software environment for statistical computing and graphics. It compiles and runs on a wide variety of UNIX platforms, Windows and macOS.

See the R project web page on "What is R?" for more information.

Sources and binaries for R and for contributed packages are available from the Comprehensive R Archive Network (CRAN).

###A-22001-2022-09-15


## Question 1

The Pareto(a, b) distribution has cdf
$$F(x)=1 − (\frac{b}{x})^ a$$
 $x ≥ b > 0,a> 0$.
Derive the probability inverse transformation $F^{-1}(U)$ and use the inverse
transform method to simulate a random sample from the Pareto(2, 2) distribution. Graph the density histogram of the sample with the Pareto(2, 2)
density superimposed for comparison.

## Answer 1

We use the inverse
transform method to simulate a random sample from the Pareto(2, 2) distribution. The method can be summarized as follows:

&nbsp;&nbsp;&nbsp;&nbsp; 1.We can derive the inverse function $F_{X}^{−1}(u)=\frac{b} {(1-u)^{\frac{1}{a}}}$.

  2. Write a command or function to compute $F_{X}^{−1}(u)$.

  3. For each random variate required:

&emsp;&emsp;$(a)$ Generate a random u from Uniform(0,1).

&emsp;&emsp;$(b)$ Deliver $x = F_{X}^{−1}(u)$.

So we can considerthe random sample x generated by the above method follows Pareto(2, 2).

The codes as follow:

```{r}
set.seed(1234)
n <- 500
a<-2
b<-2
u <- runif(n)
x <- b/(1-u)^(1/a)

```

Then,we graph the density histogram of the sample with the Pareto(2, 2)
density superimposed for comparison:

We can derive the Pareto(2, 2)
density function $f(x)=8y^{-3}$:

```{r}
hist(x, prob = TRUE) #density histogram of sample
y <- seq(0, 100, 0.01)
lines(y, 8*y^(-3)) #density curve f(x)
```

This plot shows that the sample generated by the inverse
transform method may follow the Pareto(2, 2).

```{r echo=FALSE}
rm(list=ls())
```

## Question 2
Write a function to generate a random sample of size n from the Beta(a, b)
distribution by the acceptance-rejection method. Generate a random sample
of size 1000 from the Beta(3,2) distribution. Graph the histogram of the
sample with the theoretical Beta(3,2) density superimposed.

## Answer 2
We use the acceptance-rejection method to simulate a random sample from the Beta(a, b) distribution. The method can be summarized as follows:

1. Find a random variable Y with density g satisfying f(t)/g(t) ≤ c, for
all t such that f(t) > 0. Provide a method to generate random Y.

2. For each random variate required:

&emsp;$(a)$ Generate a random y from the distribution with density g.

&emsp;$(b)$ Generate a random u from the Uniform(0, 1) distribution.

&emsp;$(c)$ If u<f(y)/(cg(y)) accept y and deliver x = y; otherwise reject y
and repeat from step (2a).

We know that the density function of Beta(a, b) is $f(x)=\frac{\Gamma(a+b)}{\Gamma(a)\Gamma(b)}x^{a-1}(1-x)^{b-1}$.

So we choose g(x)=1 for 0<x<1;and for some suitable c ;cg(x)=c can be used to generate a random sample of size n from the Beta(a, b)
distribution by the acceptance-rejection method.

For the Beta(3,2),the density function is $f(x)=12x^2(1-x)$,with $f^{'}(x)=0$.We can get the $\max f(x)=\frac{16}{9}$.

So we choose $c=\frac{16}{9}$.


```{r}
n <- 200
k <- 0 #counter for accepted
y <- numeric(n)
while (k < n) {
u <- runif(1)
x <- runif(1) #random variate from g
if (27/4*(x^2) * (1-x) > u) {
#we accept x
k <- k + 1
y[k] <- x
} }
```


```{r}
hist(y, prob = TRUE) #density histogram of sample
d <- seq(0, 1, 0.01)
m<-dbeta(d,3,2)
lines(d,m) #density curve f(x)
```

We may obtain that the density histogram of sample approximately match the theoretical  density curve f(x).



## Question 3
 Simulate a continuous Exponential-Gamma mixture. Suppose that the rate
parameter $\Lambda$ has $Gamma(r, β)$ distribution and Y has $Exp(Λ) $distribution.
That is, $(Y |Λ = λ)$ ∼ $f_Y (y|λ)$ = $λe^{−λy}$. Generate 1000 random observations
from this mixture with r = 4 and$ β = 2$.

## Answer 3

The codes that generate 1000 random observations
from this mixture as follow:
```{r}
set.seed(1234)
#generate a Exponential-Gamma mixture
n <- 200
r <- 4
beta <- 2
lambda <- rgamma(n, r, beta) #lambda is random
#now supply the sample of lambda’s as the Poisson mean
x <- rgamma(n, 1,lambda) #the mixture
```

## Question 4
 It can be shown that the mixture in Exercise 3.12 has a Pareto distribution
with cdf
$$F(y)=1 − (\frac{ β}{β + y })^ r$$
$y ≥ 0.$
(This is an alternative parameterization of the Pareto cdf given in Exercise
3.3.) Generate 1000 random observations from the mixture with r = 4 and
β = 2. Compare the empirical and theoretical (Pareto) distributions by graphing the density histogram of the sample and superimposing the Pareto density
curve.


## Answer 4

We can derive that the Pareto density function $f(x)=64(2+y)^{-5}$ for r = 4 and
β = 2.

```{r}
set.seed(1234)
#generate a Exponential-Gamma mixture
n <- 200
r <- 4
beta <- 2
lambda <- rgamma(n, r, beta) #lambda is random
#now supply the sample of lambda’s as the Poisson mean
x <- rgamma(n, 1,lambda) #the mixture
#compare with Pareto
hist(x,breaks=12,prob = TRUE)
lines(density(x))
d <- seq(0, 1000, 0.01)
lines(d,64*(2+d)^(-5),col='red')
```

 The black line and density histogram is the empirical Pareto distribution, and the red line is theoretical Pareto distribution.We may obtain that the empirical Pareto distribution approximately match the theoretical  Pareto(4,2) distribution.
 
```{r echo=FALSE}
rm(list=ls())
``` 
 
###A-22001-2022-09-23


## Question 1

For $n = 10^4, 2 × 10^4, 4 × 10^4, 6 × 10^4, 8 × 10^4$
, apply the fast sorting algorithm to randomly permuted
numbers of 1, . . . , n. 

• Calculate computation time averaged over 100 simulations, denoted by $a_n$.

• Regress $a_n$ on $t_n := n log(n)$, and graphically show the results (scatter plot and regression line).

## Answer 1
We use this code  Calculate computation time, and repeat 100 times for the average time t.
```{r}
quick_sort<-function(x){
  num<-length(x)
  if(num==0||num==1){return(x)
  }else{
    a<-x[1]
    y<-x[-1]
    lower<-y[y<a]
    upper<-y[y>=a]
    return(c(quick_sort(lower),a,quick_sort(upper)))}
}


t=rep(1,100)
i=1
for(i in 1:100)
 {test<-sample(1:1e3)
  t[i]=system.time(quick_sort(test))[1]
  i=i+1}
t1=mean(t)
```

```{r echo=FALSE}
t=rep(1,100)
i=1
for(i in 1:100)
{test<-sample(1:2e3)
t[i]=system.time(quick_sort(test))[1]
i=i+1}
t2=mean(t)

t=rep(1,100)
i=1
for(i in 1:100)
{test<-sample(1:4e3)
t[i]=system.time(quick_sort(test))[1]
i=i+1}
t3=mean(t)

t=rep(1,100)
i=1
for(i in 1:100)
{test<-sample(1:6e3)
t[i]=system.time(quick_sort(test))[1]
i=i+1}
t4=mean(t)

t=rep(1,100)
i=1
for(i in 1:100)
{test<-sample(1:8e3)
t[i]=system.time(quick_sort(test))[1]
i=i+1}
t5=mean(t)
```
By changing n in $10^4, 2 × 10^4, 4 × 10^4, 6 × 10^4, 8 × 10^4$ ,we can calculate different $a_n=(0.0217, 0.0450, 0.0896 ,0.1410, 0.1942)$,it increases along with n.


Then we  regress $a_n$ on $t_n := n log(n)$ and graph the scatter plot and regression line:

```{r}
n=c(1e3,2e3,4e3,6e3,8e3)
a=c(t1,t2,t3,t4,t5)
t=n*log(n)
b0=lm(t~a)$coefficients[1]
b1=lm(t~a)$coefficients[2]
plot(a,t)
x<-seq(0, 0.25, 0.001)
lines(x,b0+b1*x)
```

The figure shows that the regression line fit well with the scatter.So we can consider that the relationship between t and n maybe nlog(n) with something translation and scaling transformation.

## Question 2
 In Example 5.7 the control variate approach was illustrated for Monte Carlo
integration of
$$θ = \int_{0}^{1}  e^xdx$$.
Now consider the antithetic variate approach. Compute $Cov(e^U , e^{1−U} )$ and
$Var(e^U + e^{1−U} )$, where U ∼ Uniform(0,1). What is the percent reduction in
variance of $\hat{θ}$ that can be achieved using antithetic variates (compared with
simple MC)?

## Answer 2
We can calculate as follows ：

For U~U(0,1)

$E(e^U)=E(e^{1-U})=\int_{0}^{1}  e^xdx=e-1$

$E(e^{2U})=E(e^{2(1-U)})=\int_{0}^{1}  e^{2x}dx=\frac{e^2-1}{2}$

$Var(e^U)=Var(e^{1-U})=E(e^{2U})-(E(e^U))^2=\frac{e^2-1}{2}-(e-1)^2=0.2420356$


So $$Cov(e^U , e^{1−U})=E(e^Ue^{1-U})-E(e^U)E(e^{1-U})=e-(e-1)^2=-0.2342106$$

$$Var(e^U + e^{1−U} )=Var(e^U)+Var(e^{1-U})+2Cov(e^U , e^{1−U})=0.01564999$$
Then we assume $\hat{\theta}=\frac{e^U + e^{V}}{2}$ ,and U 、V are independent.

$$Var(\hat{\theta})=Var(e^U)/2=\frac{e^2-1}{4}-\frac{(e-1)^2}{2}=0.1210178$$
Then use the antithetic variate $\hat{\theta}_{an}=\frac{e^U + e^{1-U}}{2}$

$$Var(\hat{\theta}_{an})=Var(e^U + e^{1−U})/4=0.003912497$$
So the percent of reduction:

$$\frac{Var(\hat{\theta})-Var(\hat{\theta}_{an})}{Var(\hat{\theta})}=0.9676701$$


```{r echo=FALSE,include=FALSE}
v=(exp(1)^2-1)/2-(exp(1)-1)^2
c=exp(1)-(exp(1)-1)^2
2*v+2*c
a1=v/2
a2=(v+c)/2
(a1-a2)/a1
```

## Question 3
Refer to Exercise 5.6. Use a Monte Carlo simulation to estimate θ by the
antithetic variate approach and by the simple Monte Carlo method. Compute
an empirical estimate of the percent reduction in variance using the antithetic
variate. Compare the result with the theoretical value from Exercise 5.6.

## Answer 3

```{r}
set.seed(1234)
n=1000
u1=runif(n)
u2=runif(n)
u3=1-u1
t1=(mean(exp(u1))+mean(exp(u2)))/2
t2=(mean(exp(u1))+mean(exp(u3)))/2
se1=sd((exp(u1)+exp(u2))/2)
se2=sd((exp(u1)+exp(u3))/2)
p=(se1^2-se2^2)/se1^2
p
```
So we compute
an empirical estimate of the percent reduction in variance using the antithetic
variate is p=0.9678386. Comparing with the result in 5.6 p=0.9676701. They are quite close.

```{r echo=FALSE}
rm(list=ls())
```

###A-22001-2022-09-30
 
## Exercise 5.13

Find two importance functions $f_1$ and $f_2$ that are supported on $(1, ∞)$ and
are ‘close’ to

$$
g(x) =\frac{x^2}{\sqrt{2\pi}}e^{−x^{2}/2}, x>1.
$$
Which of your two importance functions should produce the smaller variance
in estimating

$$
\int_{1}^{\infty} \frac{x^2}{\sqrt{2\pi} }e^{−x^2/2}dx
$$
by importance sampling? Explain.

**Solution.** 

·Find two importance functions $f_1$ and $f_2$ that are supported on $(1, ∞)$ and
are ‘close’ to g(x).

Here we can consider $f_1$、$f_2$ follows some truncated Normal distribution and shifted  F distribution which  are supported on x > 1. 

So we choose  $$f_1(x)=\frac{e^{−(x-1)^2/2}}{\sqrt{2\pi} }/\int_{1}^{\infty}\frac{e^{−(x-1)^2/2}}{\sqrt{2\pi} }=2\frac{e^{−(x-1)^2/2}}{\sqrt{2\pi} } ,x>1.$$ It means we have a truncated Normal distribution ,i.e. $X_1$~$N(1,1)$ on $x_1>1$.

Then we choose 
$$
\begin{aligned}
f_2(x)&=\frac{\Gamma((4+4)/2)}{\Gamma(4/2)\Gamma(4/2)}(\frac{4}{4})^{\frac{4}{2}}(x-1)^{\frac{4}{2}-1}(1+\frac{4}{4}(x-1))^{-\frac{4+4}{2}}\\
&=18(x-1)x^{-4}
\end{aligned}
$$
It means if $X_2$~$f_2$ we have $(X_2-1)$~$F(4,4)$



Both are satisfied the
conditions that the support set is (1, ∞).

We graph g(x)、$f_1$ and $f_2$ so that we can compare them  directly.

```{r}
 x <- seq(1, 10, 0.01)
 y <- x^2 * exp(-x^2/2)/sqrt(2 * pi)
 plot(x, y, type = "l", ylim = c(0, 1))
 lines(x, 2*dnorm(x, 1,1), lty = 2)#Truncated Normal distribution
 lines(x, df(x - 1, 4, 4), lty = 3)#shifted F distribution
 legend("topright", inset = 0.03, legend = c("g(x)", "f_1",
  "f_2"), lty = 1:3)
```


As is shown above, $f_1$ and $f_2$ seem both are ‘close’ to g(x).

Because  variance of importance sampling  $\hat{θ}$ is $var(g(X_1)/f (X_1))/m$, which has the
minimal value 0 when g(·) = cf (·) for some constant c.

Then Compare the ratios g(x)/f(x).The plot showed as follows: 

```{r}
plot(x, y/(2*dnorm(x, 1,1)), type = "l", lty = 2,
 ylab = "",ylim = c(0, 1))
 lines(x, y/df(x - 1, 4, 4), lty = 3)
 legend("topright", inset = 0.03, legend = c("f_1", "f_2"),
 lty = 2:3)

```

The plot shows that the ratio g(x)/f(x) of truncated normal  importance function is
closer to a constant function than the shifted F distribution . So we may consider the truncated normal  importance function as it can produce the smaller variance in estimating the integral . 





## Exercise 5.15

 Obtain the stratified importance sampling estimate in Example 5.13 and compare it with the result of Example 5.10.
 
***Example 5.13 (Example 5.10, cont.)***

In Example 5.10 our best result was obtained with importance function $f_3(x) =e^{−x}/(1 − e^{−1}), 0 <x< 1.$ From 10000 replicates we obtained the estimate
$\hat{θ}$ = 0.5257801 and an estimated standard error 0.0970314.

Now divide the
interval (0,1) into five subintervals, (j/5,(j + 1)/5), j = 0, 1,..., 4.
Then on the jth subinterval variables are generated from the density
$$f_{j}(x)=\frac{5e^{−x}}{ 1 − e^{−1}} , \frac{j − 1}{ 5} <x< \frac{j}{5}.$$
(There is something wrong with this subintervals.) So we just modify it's subintervals slightly as follows:

$$f_{j}(x)=\frac{5e^{−x}}{ 1 − e^{−1}} , a_{j-1} <x< a_{j}.$$
Where $a_j=F^{-1}(\frac{j}{k})$

$$F(x)=\int_{0}^{x}f(x)dx=\frac{1-e^{-x}}{1-e^{-1}}$$

Then $$a_{j}=F^{-1}(\frac{j}{k})=-log(1-\frac{j}{k}(1-e^{-1}))$$
**Solution.**

Our aim is to estimate $\int _{0}^{1} e^{−t}/(1 + t^2)dt$. 

The integration with simple importance sampling can be written as:
$$
\begin{aligned}
\int_0^1 e^{−t}/(1 + t^2)dt& = \sum\limits_{i=1}^{k}\int_{(i-1)/k}^{i/k}e^{−t}/(1 + t^2)dt\\
&=\frac{1}{k} \sum\limits_{i=1}^{k}\int_{(i-1)/k}^{i/k}ke^{−t}/(1 + t^2)dt\\
&= \frac{1}{k}\sum\limits_{i=1}^{k}E(e^{−X_{i}}/(1 + X_{i}^2)) \\
&=\frac{1}{k}\sum\limits_{i=1}^{k}E(g_{i}(X))\\
where X ∼ U((i − 1)/k, i/k).
\end{aligned}
$$
with importance function
$$f(x) = e^{−x}/( 1 − e^{−1}) , 0 <x< 1,$$
on five subintervals, (a_{j-1} , a_{j}), j = 0, 1,..., 4. On the jth subinterval
$$f_{j}(x)=\frac{5e^{−x}}{ 1 − e^{−1}} , a_{j-1} <x< a_{j}.$$

Where
$$a_{j}=F^{-1}(\frac{j}{k})=-log(1-\frac{j}{k}(1-e^{-1}))$$
Then we have the stratified inportance sampling:


\begin{aligned}
\int_0^1 e^{−t}/(1 + t^2)dt
&=\frac{1}{k}\sum\limits_{i=1}^{k}E(g_{i}(X))\\
&=\frac{1}{k}\sum\limits_{i=1}^{k}E(g_{i}(X)/f_{i}(x))\\
where X ∼ f_{i}(x).
\end{aligned}



```{r}
set.seed(1234)
 n <- 1000
 k <- 5
 m <- n/k
 t <- numeric(k)
 va <- numeric(k)
 g <- function(x) exp(-x)/(1 + x^2)
 f <- function(x) k*(1/(1 - exp(-1))) * exp(-x)
 for (j in 1:k) {
 u <- runif(m, (j-1)/k, j/k)
 x<--log(1-(1-exp(-1))*u)
 gf <- g(x)/f(x)
 t[j] <- mean(gf)
 va[j] <- var(gf)
 }
```

```{r}
sum(t)
mean(va)
```

Without stratification :

```{r}
set.seed(234)
 n <- 1000
 k <- 1
 m <- n/k
 t <- numeric(k)
 va <- numeric(k)
 g <- function(x) exp(-x)/(1 + x^2)
 f <- function(x) (1/(exp(-(j-1)/k) - exp(-j/k))) * exp(-x)
 for (j in 1:k) {
 u <- runif(m, (j - 1)/k, j/k)
 x <- -log(exp(-(j-1)/k) - ((exp(-(j-1)/k) - exp(-j/k))) * u)#the inverse transform method
 gf <- g(x)/f(x)
 t[j] <- mean(gf)
 va[j] <- var(gf)
 }
```

```{r}
sum(t)
mean(va)
```

Comparing with the two mathods above, we can conclude that the estimate
$\hat{θ}_{sub}$ = 0.5247828 and $\hat{θ}$=0.5247008 they are quite closed .And without stratification we have a larger variance 0.009340412. And when we use stratification ,it makes variance extremly reduced: 1.72156e-05.

```{r echo=FALSE}
rm(list=ls())
```

###A-22001-2022-10-09

### Exercise 6.4

 Suppose that $X_1,...,X_n$ are a random sample from a from a lognormal distribution with unknown parameters. Construct a 95% confidence interval for
the parameter µ. Use a Monte Carlo method to obtain an empirical estimate
of the confidence level.

**Solution.** 

Since $X_1,...,X_n$ are a random sample from a from a lognormal distributiony.

We have $X=e^{Y}$~$LN(\mu,\sigma^{2})$,where $Y$~$N(\mu,\sigma^2)$.

so we can construct the confidence interval for
the parameter µ follow these steps:

1.Generate random sample $X_1,...,X_n$ following lognormal distributiony

2.Calculate $Y_1,...,Y_n$,where $Y_i=log(X_i)$and $Y_i$~$N(\mu,\sigma^2)$.

3.Use Y and normal distribution to construct the confidence interval for
the parameter µ :

$$[\bar{Y}-\sqrt{\frac{se^{2}}{n}}z_{\alpha/2},\bar{Y}+\sqrt{\frac{se^{2}}{n}}z_{\alpha/2}]$$

4.Repeat m times ,we can obtain an empirical estimate
of the confidence level.

We assume $\mu=0$,$\sigma^2=1$,then the code can be written as follows:

```{r}
set.seed(123)
n<-100
m<-1000
x<-numeric(n)
y<-numeric(n)
CIL<-numeric(m)
CIU<-numeric(m)
i=1
for (i in 1:m)
{
  x=rlnorm(n)
  y=log(x)
  ybar=mean(y)
  se=sd(y)
  s=se/sqrt(n)
  CIL[i]=ybar+s*qnorm(0.025)
  CIU[i]=ybar+s*qnorm(0.975)
  i=i+1
}

```

```{r}
mean(0>=CIL & 0<=CIU)
```
This result is very close to 0.95.





### Exercise 6.8

 Refer to Example 6.16. Repeat the simulation, but also compute the F test
of equal variance, at significance level $\hat{α}$ .
= 0.055. Compare the power of the
Count Five test and F test for small, medium, and large sample sizes. (Recall
that the F test is not applicable for non-normal distributions.)


 
***Example 6.16 ***

Example 6.16 (Count Five, cont.)
Use Monte Carlo methods to estimate the power of the Count Five test, where
the sampled distributions are $N(µ_1 = 0, σ^{2}_1 = 1)$, $N(µ_2 = 0, σ^2_2 = 1.52)$, and
the sample sizes are $n_1 = n_2 = 20$.

```{r}
count5test <- function(x, y) {
X <- x - mean(x)
Y <- y - mean(y)
outx <- sum(X > max(Y)) + sum(X < min(Y))
outy <- sum(Y > max(X)) + sum(Y < min(X))
# return 1 (reject) or 0 (do not reject H0)
return(as.integer(max(c(outx, outy)) > 5))
}
#generate samples under H1 to estimate power
m <- 1000
sigma1 <- 1
sigma2 <- 1.5
power <- mean(replicate(m, expr={
x <- rnorm(20, 0, sigma1)
y <- rnorm(20, 0, sigma2)
count5test(x, y)
}))
```

```{r}
print(power)
```


**Solution.**

We need to Compare the power of the
Count Five test and F test for small, medium, and large sample sizes. (Recall
that the F test is not applicable for non-normal distributions.)

So we choose different n=20,50,100,500,1000 for comparing.


```{r}
count5test <- function(x, y) {
X <- x - mean(x)
Y <- y - mean(y)
outx <- sum(X > max(Y)) + sum(X < min(Y))
outy <- sum(Y > max(X)) + sum(Y < min(X))
# return 1 (reject) or 0 (do not reject H0)
return(as.integer(max(c(outx, outy)) > 5))
}
#generate samples under H1 to estimate power
sigma1 <- 1
sigma2 <- 1.5
```

The code of Count Five test:

```{r}
m <- 1000
sigma1 <- 1
sigma2 <- 1.5
c5<-numeric(5)
i=1
for (n in c(20,50,100,500,1000)){
 power <- mean(replicate(m, expr={
 x <- rnorm(n, 0, sigma1)
 y <- rnorm(n, 0, sigma2)
 count5test(x, y)
 }))
  print(power)
 }
```

The code of F test:
```{r}
m <- 1000
sigma1 <- 1
sigma2 <- 1.5
c5<-numeric(5)
i=1
for (n in c(20,50,100,500,1000)){
 Ftest <- mean(replicate(m, expr={
 x <- rnorm(n, 0, sigma1)
 y <- rnorm(n, 0, sigma2)
 Fp <- var.test(x, y)$p.value
 f <- as.integer(Fp <= 0.055)
 }))
  print(Ftest)
  }
```
It can be seen that the F-test for equal variance is more powerful as it's power is higher than the Count Five test in each cases of different sample sizes. And for the F-test ,the power is approching 1 more quickly.

### Discussion

If we obtain the powers for two methods under a particular simulation setting with 10,000 experiments:
say, 0.651 for one method and 0.676 for another method. Can we say the powers are different at 0.05
level?

• What is the corresponding hypothesis test problem?

• Which test can we use? Z-test, two-sample t-test, paired-t test or McNemar test? Why?

• Please provide the least necessary information for hypothesis testing.

**Solution.**
•We Conside the hypothesis test problem:

$$H_0:p_1=p_2 \leftrightarrow H_1:p_1\neq p_2$$
•We may consider Z-test and McNemar test.

For the Z-test:we consider each experiments are independent ,so the normal approximation could be chosen.


For the McNemar test:  it's a nonparamatric methods in the homogeneity test when we have no more information about the distribution.

•
We consider the Z-test first:

For $$n_1\bar{X}\sim B(n_1,p_1),n_2\bar{Y}\sim B(n_2,p_2)$$

Consider the large sample approximation, we have:

$$\bar{X}\sim (p_1,\frac{p_1(1-p_1)}{n_1}),\bar{Y}\sim (p_2,\frac{p_2(1-p_2)}{n_2})$$

Under the null hypothesis, we have $p_1=p_2=p$

And we use the frequency $\hat{p}=\frac{n_1\bar{X}+n_2\bar{Y}}{n_1+n_2}$

then when the sample large enough:

$$U=\frac{\bar{X}-\bar{Y}}{\sqrt{\hat{p}(1-\hat{p})}\sqrt{\frac{1}{n_1}+\frac{1}{n_2}}}\sim N(0,1)$$
Then since this question $n_1=n_2=10000$,$\bar{X}=0.651$,$\bar{Y}=0.676$,$\hat{p}=(651+676)/20000=0.6635$

Caculate u:

$$u=\frac{0.676-0.651}{\sqrt(0.6635(1-0.6635))\sqrt{2/10000}}$$

```{r}
u=(0.676-0.651)/sqrt((0.6635*(1-0.6635)))/sqrt(2/10000)
u
```
since $\alpha=0.05$,$u_{\alpha/2}=1.96$,$u=3.74>1.96$,so we may reject the null hypothesis,and consider $p_1\neq p_2$


Then consider the McNemar test:

It can be seen that the contingency table is not complete ,so we simulate the result to construct the test statistics:

$$T=\frac{(x_2-y_1)^2}{(x_2+y_1)}\sim_{H_0}\mathcal{X}(1)$$
we couldn't calculate $x_2$,$y_1$ in this problem,we just have the equation：

$$x_1+x_2=676$$
$$x_1+y_1=651$$
then we just simulate the result:

since $\mathcal{X}_{0.05}(1)=3.84$
```{r}
set.seed(123)
x1=sample(0:651,100000,replace=TRUE)
x2=676-x1
y1=651-x1
z=(x2-y1)^2/(x2+y1)
mean(z)#<3.84 
mean(z>3.84)
```
the result is not reject the null hypothesis by this method,we may consider $p_1=p_2$.

It can be found that differnt methods make differnet results .But in my opinion ,I think the Z-test may be more suitable for this problem ,because the infomation for Z-test  is quite complete while we may choose simulation in McNemar test, it may result in something wrong.  
###A-22001-2022-10-14

### Exercise 7.4

Refer to the air-conditioning data set aircondit provided in the boot package. The 12 observations are the times in hours between failures of airconditioning equipment [63, Example 1.1]:

3, 5, 7, 18, 43, 85, 91, 98, 100, 130, 230, 487.

Assume that the times between failures follow an exponential model Exp(λ).
Obtain the MLE of the hazard rate λ and use bootstrap to estimate the bias
and standard error of the estimate.

**Solution.** 

Since $X_1,...,X_12$ are follow an exponential model Exp(λ).

We use MLE to obtain $\lambda$,the likelihood function:

$$L(\lambda)=\lambda^{n}e^{-\sum_{i=1}^nX_i\lambda}$$

the logarithmic likelihood function：

$$l(\lambda)=nlog\lambda-\sum_{i=1}^nX_i\lambda$$

$$\frac{dl(\lambda)}{d\lambda}=\frac{n}{\lambda}-\sum_{i=1}^nX_i=0$$
Obtain the MLE of $\lambda$:

$$\hat{\lambda}=\frac{n}{\sum_{i=1}^nX_i}=\frac{1}{\bar{X}}$$

Then we use bootstrap to estimate the bias
and standard error of the estimate.The code and result are shown as follow:

```{r}
library(boot)
x=c(3, 5, 7, 18, 43, 85, 91, 98, 100, 130, 230, 487)
lambda=1/mean(x);
B <- 1e4; set.seed(12345);lambdastar <- numeric(B)
for(b in 1:B){
xstar <- sample(x,replace=TRUE)
lambdastar[b] <- 1/mean(xstar)
}
lambda#original statistic
mean(lambdastar)-lambda# the bias
sd(lambdastar)#bootstrap standard error
```




### Exercise 7.5

 Refer to Exercise 7.4. Compute 95% bootstrap confidence intervals for the
mean time between failures 1/λ by the standard normal, basic, percentile,
and BCa methods. Compare the intervals and explain why they may differ.



**Solution.**

For the estimate of $\frac{1}{\lambda}$:

$$\hat{\theta}=\frac{1}{\hat{\lambda}}=\bar{x}$$

We can use the R-package 'boot' to calculate the confidence intervals by the standard normal, basic, percentile,
and BCa methods;the result shows as follows:

```{r}
library(boot)
x=c(3, 5, 7, 18, 43, 85, 91, 98, 100, 130, 230, 487)
set.seed(12345)
boot.mean <- function(x,i) mean(x[i])
r <- boot(x, statistic = boot.mean, R = 3000)
r
boot.ci(r, type = c("norm", "perc", "basic", "bca"))

```
We can also graph the histogram of $\hat{\theta}$:

```{r}
hist(r$t, prob = TRUE, main = "")
abline(v=r$t0,col='red',lwd=2)
```



It can be noted that the 95\% confidence interval:
Normal  method is ( 33.3, 183.0 );  Basic method is ( 23.6, 171.7 ) ;Percentile  method is  ( 44.4, 192.6 ) ;BCa  method is ( 55.9, 226.4 )  

The histogram  are showed that $\hat{\theta}$ may not approximately normal , so the normal and percentile
intervals differ. And the histogram also has displayed that it's a right-skewed distribution, so we just use a small size  sample that we could not give a good approximation when we choose CLT . 


### Exercise 7.A

 Conduct a Monte Carlo study to estimate the coverage probabilities of the
standard normal bootstrap confidence interval, the basic bootstrap confidence
interval, and the percentile confidence interval. Sample from a normal population and check the empirical coverage rates for the sample mean. Find
the proportion of times that the confidence intervals miss on the left, and the
porportion of times that the confidence intervals miss on the right.

**Solution.**

```{r}
mu<-0;b<-1;n<-500;m<-1e3;library(boot);set.seed(435)
boot.mean <- function(x,i) mean(x[i])
ci.norm<-ci.basic<-ci.perc<-matrix(NA,m,2)
for(i in 1:m){
R<-rnorm(n,0,1);
de <- boot(data=R,statistic=boot.mean, R = 1000)
ci <- boot.ci(de,type=c("norm","basic","perc"))
ci.norm[i,]<-ci$norm[2:3];ci.basic[i,]<-ci$basic[4:5]
ci.perc[i,]<-ci$percent[4:5];
}
```


The
standard normal bootstrap confidence interval:

$$( \hat{ θ} − z_{1−α/2} \hat{ se}( \hat θ), \hatθ − z_{α/2} \hat{ se}( \hat θ))$$
so we can calculate the empirical coverage rates:
```{r}
mean(ci.norm[,1]<=mu & ci.norm[,2]>=mu)
```
the basic bootstrap confidence
interval：
The (1 − α)-CI of θ is：

$$( \hatθ − F^{−1} (1 − α/2), \hat θ − F^{−1} (α/2))$$
$F^{−1} (1 − α/2)$ and $F^{ −1} (α/2)$ can be approximated by the
(1 − α/2)- and (α/2)-quantiles of
$\hatθ ^{∗} − \hat θ$:$\hatθ^{∗}_{1−α/2} −\hatθ$ and$\hatθ^{∗}_{α/2} −\hatθ$:

The resulting (1 − α)-CI is

$$(2 \hatθ − \hat{θ}^{∗}_{1−α/2} ,2 \hatθ − \hatθ^{∗}_{α/2} )$$
the empirical coverage rates：

```{r}
mean(ci.basic[,1]<=mu & ci.basic[,2]>=mu)
```

The percentile confidence interval:
$$( \hat θ^{*}_{α/2} ,\hatθ^∗_{1−α/2} )$$
the empirical coverage rates：
```{r}
mean(ci.perc[,1]<=mu & ci.perc[,2]>=mu)
```

###A-22001-2022-10-21

### Exercise 7.8

 Refer to Exercise 7.7. Obtain the jackknife estimates of bias and standard
error of $\hatθ$.

**Exercise 7.7**

Refer to Exercise 7.6. Efron and Tibshirani discuss the following example [84,
Ch. 7]. The five-dimensional scores data have a 5 × 5 covariance matrix Σ,
with positive eigenvalues $λ_1 > ··· > λ_5$. In principal components analysis,
$$θ = \frac{\lambda_{1}}{\sum\limits_{i=1}^{5}\lambda_{i}}$$
measures the proportion of variance explained by the first principal component. Let $\hatλ_1 > ··· > \hatλ_5$ be the eigenvalues of $\hatΣ$, where ˆ
Σ is the MLE of Σ.
Compute the sample estimate
$$\hatθ = \frac{\hat\lambda_{1}}{\sum\limits_{i=1}^{5}\hat\lambda_{i}}$$

**Solution.** 


Then we use jackknife to estimate the bias
and standard error of the estimate.The code and result are shown as follow:

```{r}
set.seed(123)
 library(bootstrap)
 s <- as.matrix(scor)
 n <- nrow(s)
 theta <- rep(0,n)
 m=cov(s)
 lambda <- eigen(m)$values
 theta.hat <- max(lambda)/sum(lambda)
 for (i in 1:n) {
 y <- s[-i, ]
 x <- cov(y)
 lambda <- eigen(x)$values
 theta[i] <- max(lambda)/sum(lambda)
 }
 bias <- (n - 1) * (mean(theta) - theta.hat)
 se <- sqrt((n - 1) * mean((theta - mean(theta))^2))
 
 bias
 
 se
```
So it can be noted that we use jackknife to estimate the bias 0.001069139,standard error is 0.04955231.




### Exercise 7.11

  In Example 7.18, leave-one-out (n-fold) cross validation was used to select the
best fitting model. Use leave-two-out cross validation to compare the models.



**Solution.**
We choose leave-two-out cross validation to select the best fitting model from:

$$Linear: Y = β_0 + β_1 X + \epsilon$$

$$Quadratic: Y = β_0 + β_1 X + β_2 X^2 + \epsilon$$

$$Exponential: log(Y) = β_ 0 + β_1 X + \epsilon$$

$$Log-log: log(Y) = β_0 + β_1 log(X) + \epsilon$$

Comparing the leave-two-out cross validation with leave-one-out cross validation, the leave-two-out method  shown as follows

```{r}
library(DAAG, warn.conflict = FALSE)
 attach(ironslag)
 n <- length(chemical)
 N <- choose(n, 2)
 e1 <-  numeric(N)
 m <- 1
 for (i in 1:(n - 1)) 
    for (j in (i + 1):n) 
      {
       k <- c(i, j)
       y <- magnetic[-k]
       x <- chemical[-k]
       M1 <- lm(y ~ x)
       y.h1 <- M1$coef[1] + M1$coef[2] * chemical[k]
       e1[m] <- sum((magnetic[k] - y.h1)^2)
       m=m+1
 }
 mean(e1)
```



```{r}
 n <- length(chemical)
 N <- choose(n, 2)
 e2 <-  numeric(N)
 m <- 1
 for (i in 1:(n - 1)) 
   for (j in (i + 1):n) 
     {
      k <- c(i, j)
      y <- magnetic[-k]
      x <- chemical[-k]
      M2 <- lm(y ~ x + I(x^2))
      y.h2 <- M2$coef[1] + M2$coef[2] * chemical[k] +
      M2$coef[3] * chemical[k]^2
      e2[m] <- sum((magnetic[k] - y.h2)^2)
      m=m+1
 }
 mean(e2)
```
```{r}
 n <- length(chemical)
 N <- choose(n, 2)
 e3 <-  numeric(N)
 m <- 1
 for (i in 1:(n - 1)) 
   for (j in (i + 1):n) 
     {
       k <- c(i, j)
       y <- magnetic[-k]
       x <- chemical[-k]
       M3 <- lm(log(y) ~ x)
       logy.h3 <- M3$coef[1] + M3$coef[2] * chemical[k]
       y.h3 <- exp(logy.h3)
       e3[m] <- sum((magnetic[k] - y.h3)^2)
       m=m+1
 }
 mean(e3)
```
 

```{r}
 n <- length(chemical)
 N <- choose(n, 2)
 e4 <-  numeric(N)
 m <- 1
 for (i in 1:(n - 1)) 
   for (j in (i + 1):n) 
     {
       k <- c(i, j)
       y <- magnetic[-k]
       x <- chemical[-k]
       M4 <- lm(log(y) ~ log(x))
       logy.h4 <- M4$coef[1] + M4$coef[2]*log(chemical[k])
       y.h4 <- exp(logy.h4)
       e4[m] <- sum((magnetic[k] - y.h4)^2)
       m=m+1
 }
 mean(e4)
```
 
in example 7.18 we use leave-one-out (n-fold) cross validation ,which has calculated the average squared prediction error:$e_1=19.6,e_2=17.9,e_3=18.4,e_4=20.4$So,we choose the Quadratic Model ,as it's average squared prediction error is smallest
 
Then we use leave-two-out (n-fold) cross validation to calculated the average squared prediction error:$e_1=39.14455,e_2=35.74037,e_3=36.90983,e_4=40.93436$So,we choose the Quadratic Model too ,as it also has a smallest average squared prediction error.


### Exercise 8.2

  Implement the bivariate Spearman rank correlation test for independence
[255] as a permutation test. The Spearman rank correlation test statistic can be obtained from function cor with method = "spearman". Compare the
achieved significance level of the permutation test with the p-value reported
by cor.test on the same samples.

**Solution.**

```{r}
p.spear<-function(x,y)
{
  test<-cor.test(x,y,method='spearman')
  n=length(x)
  retest<-replicate(R, expr = {
                               k <- sample(1:n)
                               cor.test(x[k], y, method =                                "spearman")$estimate
                                })
  b <- c(test$estimate, retest)
  p <- mean(as.integer(test$estimate <=
    b))
  return(list(rho.s = test$estimate, p.value = p))
}
```


Then we may compare the
achieved significance level of the permutation test with the p-value reported
by cor.test on the same samples from the Multinormal distribution,where the true $corr(X_1,X_2)=0.6$

```{r}
set.seed(123)
library(MASS)
 mu <- c(0, 0)
 Sigma <- matrix(c(1, 0.6, 0.6, 1), 2, 2)
 n <- 50
 R <- 500
 x <- mvrnorm(n, mu, Sigma)
 
```

```{r}
cor.test(x[, 1], x[, 2], method = "spearman")
p.spear(x[, 1], x[, 2])
```
The result shows that  the correlation $\rho$ estimated by spearman method is 0.5820408 which is quite close to 0.6.And p-value calculated by cor.test is 1.368e-05 while by the permutation test is 0.001996008,though the p-value are quite different,both of them may be considered to reject the null hypothesis then the correlation is not significant. 



###A-22001-2022-10-28

### Exercise 9.4

Implement a random walk Metropolis sampler for generating the standard
Laplace distribution (see Exercise 3.2). For the increment, simulate from a
normal distribution. Compare the chains generated when different variances
are used for the proposal distribution. Also, compute the acceptance rates of
each chain.

Then use the Gelman-Rubin method to monitor convergence of the chain, and run
the chain until it converges approximately to the target distribution according to $\hat{R} < 1.2$.

**Solution.** 
The standard Laplace distribution has density $$f(x) = \frac{1}{2} e^{−|x|}, x ∈ R$$

The proposal distribution follows $N(\mu,\sigma^2)$

For the Metropolis sampler: g(r|s) = g(s|r) so that,$$α(r,s) = min \{f (s)/f (r),1\}=min\{e^{|r|-|s|},1\}$$

Generating the standard
Laplace distribution:
```{r}
    set.seed(1234)
    f<-function(x){
      d=1/2*exp(-abs(x))
      return(d)
    }
    
    rw.Metropolis <- function(n, sigma, x0, N) {
        x <- numeric(N)
        x[1] <- x0
        u <- runif(N)
        k <- 0
        for (i in 2:N) {
            y <- rnorm(1, x[i-1], sigma)
                if (u[i] <= (f(y) / f(x[i-1])))
                    x[i] <- y  
                else {
                    x[i] <- x[i-1]
                    k <- k + 1
                }
            }
        return(list(x=x, k=k))
    }

    n <- 4  #degrees of freedom for target Student t dist.
    N <- 5000
    sigma <- c( .5,1,3,5)

    x0 <- 25
    rw1 <- rw.Metropolis(n, sigma[1], x0, N)
    rw2 <- rw.Metropolis(n, sigma[2], x0, N)
    rw3 <- rw.Metropolis(n, sigma[3], x0, N)
    rw4 <- rw.Metropolis(n, sigma[4], x0, N)

```


The acceptance rates of
each chain and graph the chain：


the Gelman-Rubin method:
```{r}
Gelman.Rubin <- function(psi) {
        # psi[i,j] is the statistic psi(X[i,1:j])
        # for chain in i-th row of X
        psi <- as.matrix(psi)
        n <- ncol(psi)
        k <- nrow(psi)

        psi.means <- rowMeans(psi)     #row means
        B <- n * var(psi.means)        #between variance est.
        psi.w <- apply(psi, 1, "var")  #within variances
        W <- mean(psi.w)               #within est.
        v.hat <- W*(n-1)/n + (B/n)     #upper variance est.
        r.hat <- v.hat / W             #G-R statistic
        return(r.hat)
        }
```

Use the Gelman-Rubin method to monitor convergence of the chain:
```{r}
s <- c(0.5,1,3,5)     #parameter of proposal distribution
    n <- 4          #number of chains to generate
    N <- 6000     #length of chains
    b <- 1000       #burn-in length

    #choose overdispersed initial values
    x0 <- c(-10, -5, 5, 10)
set.seed(12345)
for(k in 1:n){
    X <- matrix(0, nrow=n, ncol=N)
    for (i in 1:n)
        X[i,] <- rw.Metropolis(n,s[k],x0[i],N)$x
   psi <- t(apply(X, 1, cumsum))
    for (i in 1:nrow(psi))
        psi[i,] <- psi[i,] / (1:ncol(psi))
      #plot the sequence of R-hat statistics
    rhat <- rep(0, N)
    for (j in (b+1):N)
        rhat[j] <- Gelman.Rubin(psi[,1:j])
    plot(rhat[(b+1):N], type="l", xlab=bquote(sigma == .(round(s[k],3))), ylab="R")
    abline(h=1.2, lty=2)
}
```


Run
the chain until it converges approximately to the target distribution according to $\hat{R} < 1.2$：
```{r}
s <- c(0.5,1,3,5)     #parameter of proposal distribution
    n <- 4          #number of chains to generate
    N <- 6000      #length of chains
    b <- 1000       #burn-in length
   par(mfrow=c(2,2))
    x0 <- 10
set.seed(12345)
rw1 <- rw.Metropolis(n, s[1], x0, N)
    rw2 <- rw.Metropolis(n, s[2], x0, N)
    rw3 <- rw.Metropolis(n, s[3], x0, N)
    rw4 <- rw.Metropolis(n, s[4], x0, N)
    refline <- qt(c(.025, .975), df=n)
    rw <- cbind(rw1$x, rw2$x, rw3$x,  rw4$x)
```





### Exercise 9.7

  Implement a Gibbs sampler to generate a bivariate normal chain $(X_t, Y_t)$
with zero means, unit standard deviations, and correlation 0.9. Plot the
generated sample after discarding a suitable burn-in sample. Fit a simple
linear regression model $Y = β_0 + β_1X$ to the sample and check the residuals
of the model for normality and constant variance.

Then use the Gelman-Rubin method to monitor convergence of the chain, and run
the chain until it converges approximately to the target distribution according to $\hat{R} < 1.2$.

**Solution.**
```{r}
N <- 5000 #length of chain
burn <- 1000 #burn-in length
X <- matrix(0, N, 2) #the chain, a bivariate sample
rho <- .9 #correlation
mu1 <- 0
mu2 <- 0
sigma1 <- 1
sigma2 <- 1
x01<-mu1 #initialize
x02<-mu2
###### generate the chain #####
NG<-function(N,mu1,mu2,sigma1,sigma2,rho,x01,x02){
  s1 <- sqrt(1-rho^2)*sigma1
  s2 <- sqrt(1-rho^2)*sigma2
  X[1,]<c(x01,x02)
    for (i in 2:N) {
                    x2 <- X[i-1, 2]
                    m1 <- mu1 + rho * (x2 - mu2) * sigma1/sigma2
                    X[i, 1] <- rnorm(1, m1, s1)
                    x1 <- X[i, 1]
                    m2 <- mu2 + rho * (x1 - mu1) * sigma2/sigma1
                    X[i, 2] <- rnorm(1, m2, s2)
    }
  X1<-X[,1]
X2<-X[,2]
return(list(X1=X1,Y1=X2))
}
```

Generated sample after discarding a suitable burn-in which we choose b=1000,then plot the burn-in sample:
```{r}
b <- burn + 1
x<-NG(N,mu1,mu2,sigma1,sigma2,rho,x01,x02)$X1
y<-NG(N,mu1,mu2,sigma1,sigma2,rho,x01,x02)$Y1
X<-x[b:N]
Y<-y[b:N]
```


```{r}
plot(X,type='l',col=1,lwd=2,xlab='Index',ylab='Random numbers')
lines(Y,col=2,lwd=2)
legend('bottomright',c(expression(X),expression(Y)),col=1:2,lwd=2)
```
Fit a simple
linear regression model $Y = β_0 + β_1X$ to the sample:

```{r}
l=lm(Y~X)
summary(lm(Y~X))
```
It can be seen that , if we asumme that Y = 0.9X and Var(X) = V ar(Y ) = 1, then Cor(X, Y ) =
Cor(X, 0.9X)=0.9,we may have a fitted model.

The qq plot of the residual of the generated chain (after discarding the burn-in sample) is
shown below. 

```{r}
 qqnorm(l$res, cex = 0.25)
 qqline(l$res)
```
 The QQ plot of residuals is consistent with the
normal error assumption of the linear model.So we may accept the normality assumption.


For residual plots can be
generated as shown below.
```{r}
 plot(l$fit, l$res, cex = 0.25)
 abline(h = 0)
```
The plot of residuals vs fits suggests that the error variance is constant as it dosen't have any tendency .


Use the Gelman-Rubin method to monitor convergence of the chain:

```{r}

N <- 6000 #length of chain
b <- 1000 #burn-in length
X <- matrix(0, N, 2) #the chain, a bivariate sample
rho <- .9 #correlation
mu1 <- 0
mu2 <- 0
sigma1 <- 1
sigma2 <- 1
x01<-c(-1.5,-0.5,0,1,2) #initialize
x02<-c(-1.5,-0.5,0,1,2)
n<-5
    #choose overdispersed initial values
set.seed(23456)
    x <- matrix(0, nrow=n, ncol=N)
    y <- matrix(0, nrow=n, ncol=N)
    for (i in 1:n)
       { 
      x[i,] <- NG(N,mu1,mu2,sigma1,sigma2,rho,x01[i],x02[i])$X1
      y[i,] <- NG(N,mu1,mu2,sigma1,sigma2,rho,x01[i],x02[i])$Y1}
   psi1 <- t(apply(x, 1, cumsum))
   psi2 <- t(apply(y, 1, cumsum))
    for (i in 1:nrow(psi1))
        {psi1[i,] <- psi1[i,] / (1:ncol(psi1))
         psi2[i,] <- psi2[i,] / (1:ncol(psi2))
         }
      #plot the sequence of R-hat statistics
    rhat1 <- rep(0, N)
    rhat2 <- rep(0, N)
    for (j in (b+1):N)
        {rhat1[j] <- Gelman.Rubin(psi1[,1:j])
        rhat2[j] <- Gelman.Rubin(psi2[,1:j])
    }
    plot(rhat1[(b+1):N], type="l", xlab='X', ylab="R")
    abline(h=1.2, lty=2)
    plot(rhat2[(b+1):N], type="l", xlab='Y', ylab="R")
    abline(h=1.2, lty=2)
```
It can be noted that the convergence rate of chain X is slower than chain Y

```{r}
N <- 6000 #length of chain
b <- 1000 #burn-in length
X <- matrix(0, N, 2) #the chain, a bivariate sample
rho <- .9 #correlation
mu1 <- 0
mu2 <- 0
sigma1 <- 1
sigma2 <- 1
x2 <- NG(N,mu1,mu2,sigma1,sigma2,rho,0,0)$X1
y2 <- NG(N,mu1,mu2,sigma1,sigma2,rho,0,0)$Y1
    refline <- qt(c(.025, .975), df=n)
    c1<-x2[(b+1):N]
        plot(c1, type="l",
             ylab="X", ylim=range(c1))
        abline(h=refline)
        
    c2<-y2[(b+1):N]
    plot(c2, type="l",
             ylab="Y", ylim=range(c2))
        abline(h=refline)
```




###A-22001-2022-11-04

### HW1

The test of Intermediary effect

$$H_0: \alpha\beta=0  \leftrightarrow   H_1:\alpha\beta\neq0$$
the test statistics is$$T=\frac{\hat{\alpha}\hat{\beta}}{\hat{se}(\hat{\alpha}\hat{\beta})}$$
Design a randomized simulation experiment,consider the performance of the above three permutation test methods.

Consider the model:

$$M=a_M+\alpha X+e_M$$

$$Y=a_Y+\beta M+\gamma X+e_Y$$
$e_M,e_Y\sim N(0,1)$

How to design the permutation that we can proof :

$$(1)\alpha=0$$
$$(2)\beta=0$$
$$(3)\alpha=0 and \beta=0$$

when we consider that:

$$① \alpha=0,\beta=1; ②  \alpha=1,\beta=0;③\alpha=0,\beta=0$$

**Solution.** 
```{r}
library(mediation)
```



```{r}
set.seed(2345)
N=50
em<-rnorm(N)
ey<-rnorm(N)
X<-rnorm(N)
r=1
am=1
ay=2

per <- function(z, ix, dims) {
p <- dims[1] 
q <- dims[2] 
d <- p + q 
x <- z[ 1:p,] 
y <- z[-(1:p),ix] 
return(c(X1=x,Y1=y)) }

#for only X independent with M;
perm1<-function(X,M,Y,n){
  a<-lm(M~X)
  b<-lm(Y~M+X)
  re=mediate(a,b,treat='X',mediator='M',sims=20)
  c=re$d0
  se=sd(re$d0.sim)
  T0=c/se
  i=1
  t<-numeric(n)
  for(i in 1:n){
    X<- sample(X);
    k <- sample(1:length(Y))
    Y <- Y[k]; 
    M <- M[k];
    a<-lm(M~X)
    b<-lm(Y~M+X)
    re=mediate(a,b,treat='X',mediator='M',sims=20)
    c=re$d0
    se=sd(re$d0.sim)
    t[i]=c/se
  }
  p=mean(abs(c(T0,t))>=abs(T0))
  return(p)
}


#for only M independent with Y;
perm2<-function(X,M,Y,n){
  a<-lm(M~X)
  b<-lm(Y~M+X)
  re=mediate(a,b,treat='X',mediator='M',sims=20)
  c=re$d0
  se=sd(re$d0.sim)
  T0=c/se
  i=1
  t<-numeric(n)
  for(i in 1:n){
    k <- sample(1:length(X))
    X <- X[k]; 
    M <- M[k];
    Y<- sample(Y);
    a<-lm(M~X)
    b<-lm(Y~M+X)
    re=mediate(a,b,treat='X',mediator='M',sims=100)
    c=re$d0
    se=sd(re$d0.sim)
    t[i]=c/se
  }
  p=mean(abs(c(T0,t))>=abs(T0))
  return(p)
}


#for X independent with M;M independent with y
perm3<-function(X,M,Y,n){
  a<-lm(M~X)
  b<-lm(Y~M+X)
  re=mediate(a,b,treat='X',mediator='M',sims=20)
  c=re$d0
  se=sd(re$d0.sim)
  T0=c/se
  i=1
  t<-numeric(n)
  for(i in 1:n){
    X<- sample(X);
    M<-M
    Y<-sample(Y);
    a<-lm(M~X)
    b<-lm(Y~M+X)
    re=mediate(a,b,treat='X',mediator='M',sims=20)
    c=re$d0
    se=sd(re$d0.sim)
    t[i]=c/se
  }
  p=mean(abs(c(T0,t))>=abs(T0))
  return(p)
}



```
$① \alpha=0,\beta=1;$:

```{r}
n=100
alpha=0;beta=1;
M<-am+alpha*X+em
Y<-ay+beta*M+r*X+ey
p1=perm1(X,M,Y,n)#model1
p2=perm2(X,M,Y,n)#model2
p3=perm3(X,M,Y,n)#model3
c(p1,p2,p3)
```
we choose three permutation test methods show above. All the p-value are large enough(>0.05) that we can't reject the null hypothesis while p2=0.08910891 seem small.


$②  \alpha=1,\beta=0;$
```{r}
n=50
alpha=1;beta=0;
M<-am+alpha*X+em
Y<-ay+beta*M+r*X+ey
p1=perm1(X,M,Y,n)#model1
p2=perm2(X,M,Y,n)#model2
p3=perm3(X,M,Y,n)#model3
c(p1,p2,p3)
```
All the p-value are large enough(>0.05) that we can't reject the null hypothesis while p3=0.07920792 seem small.


$③\alpha=0,\beta=0$
```{r}
n=50
alpha=0;beta=0;
M<-am+alpha*X+em
Y<-ay+beta*M+r*X+ey
p1=perm1(X,M,Y,n)#model1
p2=perm2(X,M,Y,n)#model2
p3=perm3(X,M,Y,n)#model3
c(p1,p2,p3)
```

All the p-value are large that we can't reject the null hypothesis.


### HW2

  Consider the model:
  
  $$P(Y_1|X_1,X_2,X_3)=expit(\alpha+b_1X_1+b_2X_2+b_3X_3)$$

$X_1\sim P(1)$,$X_2\sim Exp(1)$,$X_3\sim B(1,0.5)$

Write a function which we imput $N,b_1,b_2,b_3,f_0$ then we may get the output $\alpha$

We use the function and choose $N=10^6,b_1=0,b_2=1,b_3=-1$

$f_0=0.1,0.01,0.001,0.0001$

graph the scatter plot of $f_0$ vs $\alpha$

**Solution.**
Write the function which can help us calculate $\alpha$:
```{r}
set.seed(12345)
x1 <- rpois(N,1);x2<-rexp(N,1); x3 <- rbinom(N,1,0.5)
s<-function(x1,x2,x3,N,f0){
g <- function(alpha){
tmp <- exp(-alpha-b1*x1-b2*x2-b3*x3)
p <- 1/(1+tmp)
mean(p) - f0
}
solution <- uniroot(g,c(-15,5))
round(unlist(solution),5)[1:3]
return(solution$root)
}
```

Choose $N=10^6,b_1=0,b_2=1,b_3=-1$

$f_0=0.1,0.01,0.001,0.0001$,then have $\alpha$ solved as follows:
```{r}
N <- 1e6; b1 <- 0; b2 <- 1;b3<- -1;
alpha<-numeric(4)
f1<-c(0.1,0.01,0.001,0.0001)
i=1
for(i in 1:4)
{
  f0<-f1[i]
alpha[i] <- s(b1,b2,b3,N,f0)
i=i+1
}
alpha
```

graph the scatter plot of $f_0$ vs $\alpha$:
```{r}
plot(f1,alpha,xlab='f0')
plot(-log(f1),alpha,xlab='-log(f0)')
```
It can be noted that $\alpha$ increases along with $f_0$,and decrease with $-log(f_0)$.

###A-22001-2022-11-11

### Class Work

$X_1,……,X_n$ $\sim_{iid}$ $Exp(\lambda)$.With some information missed ,we just know that $X_i\in (u_i,v_i)$,where $u_i<v_i$,and they are constant.

(1)Maximize the likelihood function of observation and use EM algorithm to estimate $\lambda$ .Prove both of them are equal.

(2)We have some observation(n=10):(11,12),(8,9),(27,28),(13,14),(16,17),(0,1),(23,24),(10,11),(24,25),(2,3).Use them realize two methods show above to estimate $\lambda$.
 

**Solution.** 

(1)

The likelihood function with observation:

$$L_o(\lambda)=\prod_{i=1}^{n}P(u_i<X_i<v_i)=\prod_{i=1}^{n}(e^{-\lambda u_i}-e^{-\lambda v_i})$$
The log-likelihood function:

$$l_o(\lambda)=\sum_{i=1}^{n}log(e^{-\lambda u_i}-e^{-\lambda v_i})$$
Derive $l_o$ with respect to $\lambda$ and equal to 0,Then the MLE of $\lambda$ is the solution of the equation shown as follows:

$$\sum_{i=1}^{n}\frac{v_ie^{-\lambda v_i}-u_ie^{-\lambda u_i}}{e^{-\lambda u_i}-e^{-\lambda v_i}}=0$$
While this equation is hardly to solve ,we may choose EM:

The likelyhood function with completed data:

$$L_c(\lambda)=\prod_{i=1}^{n} f(X_i)=\lambda^{n}e^{-\sum_{i=1}^{n}\lambda X_i}$$
The log- likelyhood function:

$$l_c(\lambda)=nlog(\lambda)-\sum_{i=1}^{n}\lambda X_i$$
E-step:We may maximize the conditional expectation of $l_c(\lambda)$:

$$E_{\lambda^{0}}(l_c(\lambda)|u_i<X_i<v_i,i=1…n)=nlog(\lambda)-\sum_{i=1}^{n}\lambda \hat{X_i}$$
Where $\hat{X_i}=E_{\lambda^{0}}(X_i|u_i<X_i<v_i,i=1…n)$

M-step:We can derive that:

$$\lambda^{(1)}=\frac{n}{\sum_{i=1}^{n}\hat{X_i}}$$
repeat that until the error small enough so that $\lambda$ convergence.

We may need to calculate $\hat{X_i}$:

since$$\hat{X_i}=E_{\lambda^{0}}(X_i|u_i<X_i<v_i,i=1…n)$$

$$f(x|u_i<X_i<v_i,\lambda^{0})=\frac{\lambda^{0}e^{-\lambda^{0}x}I_{[u_i,v_i]}}{e^{-\lambda^{0} u_i}-e^{-\lambda^{0} v_i}}$$
$$\hat{X_i}=\int xf(x|u_i<X_i<v_i,\lambda^{0})dx=\frac{u_ie^{-\lambda^{0} u_i}-v_ie^{-\lambda^{0} v_i}}{e^{-\lambda^{0} u_i}-e^{-\lambda^{0} v_i}}+\frac{1}{\lambda^0}$$
then
$$\lambda^{(k+1)}=\frac{n\lambda^{(k)}}{\sum_{i=1}^{n}(\lambda^{(k)}\frac{u_ie^{-\lambda^{(k)} u_i}-v_ie^{-\lambda^{(k)} v_i}}{e^{-\lambda^{(k)} u_i}-e^{-\lambda^{(k)} v_i}}+1)}$$
We may assume $\lim\lambda^{(k)}=\lambda$

then $$\lambda=\frac{n\lambda}{\sum_{i=1}^{n}(\lambda\frac{u_ie^{-\lambda u_i}-v_ie^{-\lambda v_i}}{e^{-\lambda u_i}-e^{-\lambda v_i}}+1)} \Rightarrow \sum_{i=1}^{n}\frac{u_ie^{-\lambda u_i}-v_ie^{-\lambda v_i}}{e^{-\lambda u_i}-e^{-\lambda v_i}}=0$$
It means $\lambda^{(k)}$ is convergence,and it may converge to $\lambda_{ML}$ ,which satisfies $\sum_{i=1}^{n}\frac{u_ie^{-\lambda u_i}-v_ie^{-\lambda v_i}}{e^{-\lambda u_i}-e^{-\lambda v_i}}=0$

We may have the recursive :

$$\lambda^{(k+1)}=\frac{n}{\sum_{i=1}^{n}(\frac{u_ie^{-\lambda^{(k)} u_i}-v_ie^{-\lambda^{(k)} v_i}}{e^{-\lambda^{(k)} u_i}-e^{-\lambda^{(k)} v_i}}+\frac{1}{\lambda^{(k)}})}=f(\lambda^{k})$$
Then 

$$\frac{df(x)}{dx}=\frac{n(\sum_{i=1}^{n}\frac{(u_i^2e^{-x u_i}-v_i^2e^{-x v_i})(e^{-x u_i}-e^{-x v_i})+(u_ie^{-x u_i}-v_ie^{-x v_i})^2}{(e^{-x u_i}-e^{-x v_i})^2}+\frac{1}{x^2})}{(\sum_{i=1}^{n}(\frac{u_ie^{-x u_i}-v_ie^{-x v_i}}{e^{- xu_i}-e^{-x v_i}}+\frac{1}{x}))^2}$$
we may have $0<f^{'}(x)<\frac{1}{n}<1$ since $\sum_{i=1}^{n}\frac{u_ie^{-\lambda u_i}-v_ie^{-\lambda v_i}}{e^{-\lambda u_i}-e^{-\lambda v_i}}\rightarrow0$ then under the contraction mapping theorem:

$$|\lambda^{k+1}-\lambda^{k}|\leq|f^{'}(x)| |\lambda^{k}-\lambda^{k-1}|\leq|f^{'}(x)|^{k} |\lambda^{1}-\lambda^{0}|\rightarrow0$$
While $n\rightarrow0$.i.e. $\lambda^{k}$ estimated by EM algorithm convergence to $\hat{\lambda}_{ML}$ estimated by ML.

(2).

Estimate $\lambda$ by two methods with 10 observed data：



```{r}
library(stats4)
u=c(11,8,27,13,16,0,23,10,24,2)
v=c(12,9,28,14,17,1,24,11,25,3)
lo<-function(lambda){
  sum=0
  i=1
  for(i in 1:10){
    sum=sum+(v[i]*exp(-lambda*v[i])-u[i]*exp(-lambda*u[i]))/(exp(-lambda*u[i])-exp(-lambda*v[i]))
    i=i+1
  }
  sum
}
sol <- uniroot(lo,c(0,5))
sol
```
EM:
```{r}
lambda0=1
e=1
n=10

while (e>10^(-6)){
  lambda=n/(-lo(lambda0)+n/lambda0)
  e=abs(lambda-lambda0)
  lambda0=lambda
}
lambda0
```
We estimate $\lambda$ by using ML function is  0.07196821;
and EM algorithm is  0.0719735.Both of them are quite closed.


```{r}
rm(list=ls()) 
```


### Exercise 2.1.3(4)

  
Why do you need to use unlist() to convert a list to an atomic
vector? Why doesn’t as.vector() work?


**Solution.**

We need to get rid of the nested structure.

Where possible the list elements are coerced to a common mode during the unlisting, and so the result often ends up as a character vector. Vectors will be coerced to the highest type of the components in the hierarchy NULL < raw < logical < integer < double < complex < character < list < expression: pairlists are treated as lists.

A list is a (generic) vector, and the simplified vector might still be a list (and might be unchanged). Non-vector elements of the list (for example language elements such as names, formulas and calls) are not coerced, and so a list containing one or more of these remains a list. (The effect of unlisting an lm fit is a list which has individual residuals as components.) Note that unlist(x) now returns x unchanged also for non-vector x, instead of signalling an error in that case.



### Exercise 2.1.3(5)

 Why is 1 == "1" true? Why is -1 < FALSE true? Why is "one"
< 2 false?

**Solution.**
These are all functions which coerco their arguments to character , double and character.

1 == "1" is TRUE 

"FALSE==0" is TRUE then -1 < FALSE true

'one' comes after '2' in ASCⅡ


### Exercise 2.3.1(1)

 What does dim() return when applied to a vector?

**Solution.**

it may return " NULL "



### Exercise 2.3.1(2)

 If is.matrix(x) is TRUE, what will is.array(x) return?

**Solution.**

It will also return "TRUE".

because is.matrix returns TRUE if x is a vector and has a "dim" attribute of length 2 and FALSE otherwise.i.e.A two-dimentional array is the same thing as a matrix.



### Exercise 2.4.5(1)

 What attributes does a data frame possess?

**Solution.**

A data frame posess names,row.names,class.



### Exercise 2.4.5(2)

 What does as.matrix() do when applied to a data frame with
columns of different types?

**Solution.**

From "?as.matrix":

as.matrix is a generic function. The method for data frames will return a character matrix if there is only atomic columns and any non-(numeric/logical/complex) column, applying as.vector to factors and format to other non-character columns. Otherwise, the usual coercion hierarchy (logical < integer < double < complex) will be used, e.g., all-logical data frames will be coerced to a logical matrix, mixed logical-integer will give a integer matrix, etc.



### Exercise 2.4.5(3)

 Can you have a data frame with 0 rows? What about 0
columns?

**Solution.**

Yes.You can creat a data frame with 0 rows and 0 columns.Also both of them are 0:

```{r}
iris[FALSE,]#0 rows

iris[FALSE,]#0 columns

iris[FALSE,FALSE]#0 columns and 0 rows

```




###A-22001-2022-11-18

### Exercises 2

The function below scales a vector so it falls in the range [0,
1]. How would you apply it to every column of a data frame?
How would you apply it to every numeric column in a data
frame?

```{r}
scale01 <- function(x) {
rng <- range(x, na.rm = TRUE)
(x - rng[1]) / (rng[2] - rng[1])
}
```
**Solution.** 

 We can use the if() “function”,since we want to return non-numeric input columns; and we choose the data "cars" from R:

```{r}
car<-data.frame(lapply(cars, function(x) if (is.numeric(x)) scale01(x) else x))
head(car)
tail(car)
```


```{r}
rm(list=ls()) 
```


### Exercise 1
Use vapply() to:

(a) Compute the standard deviation of every column in a numeric data frame.

(b) Compute the standard deviation of every numeric column
in a mixed data frame. (Hint: you’ll need to use vapply()
twice.)

**Solution.**

As a numeric data.frame we choose 'trees' from R:

```{r}
vapply(trees, sd, numeric(1))
```

As a mixed data.frame we choose 'iris' from R:

```{r}
vapply(iris[vapply(iris, is.numeric, logical(1))],
       sd, 
       numeric(1))
```




```{r}
rm(list=ls()) 
```
### Homework3

 Implement a Gibbs sampler to generate a bivariate normal chain $(X_t, Y_t)$ with zero means, unit standard
deviations, and correlation 0.9.

• Write an Rcpp function.

• Compare the corresponding generated random numbers with pure R language using the function
“qqplot”.

• Compare the computation time of the two functions with the function “microbenchmark”.

**Solution.**
```{r}
library(Rcpp)
```

1.You can see Rcpp function of Gibbs sampler in the file "gibbsC.cpp$

2.
Gissbs sample generaed by R language:
```{r}
gibbsR <- function(N, thin) {
  mat <- matrix(nrow = N, ncol = 2)
  x <- y <- 0
  for (i in 1:N) {
    for (j in 1:thin) {
      x <- rnorm(1, 0.9*y, sqrt(1-0.9^2))
      y <- rnorm(1, 0.9*x, sqrt(1-0.9^2))
    }
    mat[i, ] <- c(x, y)
  }
  mat
}
```

Compare the corresponding generated random numbers with pure R language using the function
“qqplot”:
```{r}
set.seed(2345)
library(microbenchmark)
library(StatComp22001)
gibbR=gibbsR(200,1)
gibbC=gibbsC(200,1)
X1=gibbR[,1]
Y1=gibbR[,2]
X2=gibbC[,1]
Y2=gibbC[,2]
qqplot(X1,Y1)
qqplot(X2,Y2)
qqplot(X1,X2)
qqplot(Y1,Y2)

```
Comparing the corresponding generated random numbers of the two functions. It could be noted that they are in the linear relationships.

```{r}
library(microbenchmark)
ts <- microbenchmark(gibbR=gibbsR(200,1),
gibbC=gibbsC(200,1))
summary(ts)[,c(1,3,5,6)]
```
It can be seen that the computation time of the two functions ,gibbC are quicker than gibbR.


